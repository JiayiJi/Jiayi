<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiayi Ji on Jiayi Ji</title>
    <link>/zh/</link>
    <description>Recent content in Jiayi Ji on Jiayi Ji</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2017 Jiayi Ji</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/zh/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>统计博客圈的书剑恩仇</title>
      <link>/zh/post/2018-03-05-blog-debate/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2018-03-05-blog-debate/</guid>
      <description>&lt;p&gt;有人的地方就有江湖。&lt;/p&gt;

&lt;p&gt;推动科学进步的是学术争论，大家围坐一席以数据与逻辑为工具互相质疑，寻求共识。但事实上这个过程中并不缺乏个人或群体情感的介入，这一方面是现代科研职业化所导致的拿钱吃饭，另一方面则是科研人员自身的主观好恶。这一点在统计学家的博客上展示的淋漓尽致，虽然在学术期刊里发评论比较正式，但在预印本、数据共享与可重复性研究的大趋势下，越来越多的统计学家选择时效性更高的非同行评议的博客来对科学进展进行评论。借助这些社交媒体，我们也可以一窥他们对学术观点的爱恨情仇，也许有人不屑于这些主观性比较强的评论，但从学术交流的角度出发，如果我们仅仅通过学术期刊与会议交流学术观点，由于存在审稿与运作周期，很多共识会消耗大量的传播成本来达成，这不仅与信息时代脱节，也会造成资源浪费。下面我们看些案例感受下国外学术界在博客这一媒介上的观点交锋：&lt;/p&gt;

&lt;p&gt;案例一：“主观”的贝叶斯方法&lt;/p&gt;

&lt;p&gt;我们哥伦比亚大学的 Andrew Gelman 的博客可以算得上是个火药桶了，他本身主张贝叶斯学派，而赶巧贝叶斯学派跟频率学派可以算得上科研数据分析里哲学思想差异最大的两派，起码按我的粗浅认识是根本无法调和的，所以即便实用上甚至算法上都差异不大，想对这两种思想和稀泥基本都会被 Gelman 教授无情嘲讽，如果你还打算说贝叶斯不好，基本上会被博文讨伐。当然，也不是所有人都有这个待遇，同方舟子的做法类似， Gelman 教授基本也是逮着大鱼去坑。需要提醒的是他可不是方舟子那种十几年不做科研的学术圈外人士，其本人是我们哥伦比亚大学应用统计中心的主任，其团队的研究领域十分广阔。顺带一提，著名贝叶斯统计软件 stan 就出自这个团队。&lt;/p&gt;

&lt;p&gt;这次事情的起因是卡内基·梅隆大学著名的 Larry Wasserman 教授（2016年当选美国国家科学院院士）在接受一个博客&lt;a href=&#34;https://errorstatistics.com/2013/12/28/wasserman-on-wasserman-update-december-28-2013/&#34; target=&#34;_blank&#34;&gt;采访&lt;/a&gt;时对频率学派与贝叶斯学派下了个定义：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wish people were clearer about what Bayes is/is not and what  frequentist inference is/is not. Bayes is the analysis of subjective  beliefs but provides no frequency guarantees. Frequentist inference  is about making procedures that have frequency guarantees but makes no pretense of representing anyone’s beliefs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Gelman 教授对其频率学派的观点没啥意见，但那个 &amp;ldquo;subjective&amp;rdquo; 直接引爆了火药桶。而按照 Gelman 的定义，贝叶斯方法应该是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Using inference from the posterior distribution, p(theta|y)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;特别的，他还认为：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Science is always full of subjective human choices, and it’s always about studying larger questions that have an objective reality.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;坦白说这个看法是比较符合科学史的，虽然当今科学理论体系逻辑上相对完备（先排除下哥德尔跟量子力学），但其发展确实很曲折，在实验数据跟统计决策成为主流之前，很多理论在发现或提出时主流科学家并不接受，有的是逻辑上不接受（很多新理论完全不容于旧理论），有的则属于威权集团打压，可以说相当主观。&lt;/p&gt;

&lt;p&gt;但在后面的论述中，Gelman 教授就开启嘲讽技能了，Larry 认为在高维数据处理中贝叶斯方法没意义无法解释，Gelman 教授则反驳说他觉得除了贝叶斯方法别的方法也都是解释不通的，并且他认为 Larry 自己不懂贝叶斯还瞎定义是十分不妥的。不得不说这段论述很没营养，跟小学生吵架差不多。紧接着 Gelman 教授又提到主观确实是贝叶斯方法的一部分但不是全部，那频率学派是不是可以说成“简单随机采样的技术”，科学研究范围在拓展，各种方法也在发展，贝叶斯方法可以研究客观问题。这个说法也比较中肯，接下来 Gelman 教授又开启了挖坟模式，他把 Larry 08年到13年关于贝叶斯方法中随机性看法的转变给列了出来，紧接着又说我也有这个转变过程。但文章最后他又翻了 Larry 对经济学家的旧账，认为他存在个人偏见。&lt;/p&gt;

&lt;p&gt;看起来这个文章似乎比较正常，但这篇博文真正有趣的是评论，基本上集中了当今统计学中各路高手，下面是个不完全名单：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nick Cox 杜伦大学 Stata 元老级开发者&lt;/li&gt;
&lt;li&gt;Larry Wasserman 卡内基·梅隆大学教授 当事人&lt;/li&gt;
&lt;li&gt;Deborah G. Mayo 宾夕法尼亚大学教授 采访 Larry 的人 errorstatistics.com 博主&lt;/li&gt;
&lt;li&gt;Kevin Dick 斯坦福毕业 创业者 possibleinsight.com 博主&lt;/li&gt;
&lt;li&gt;Judea Pearl UCLA 教授 &lt;a href=&#34;http://causality.cs.ucla.edu/blog/&#34; target=&#34;_blank&#34;&gt;http://causality.cs.ucla.edu/blog/&lt;/a&gt; 博主&lt;/li&gt;
&lt;li&gt;Christian Hennig 伦敦大学学院教授&lt;/li&gt;
&lt;li&gt;Norm Matloff UC Davis 教授&lt;/li&gt;
&lt;li&gt;Brendan K O&amp;rsquo;Rourke 都柏林理工教授 &lt;a href=&#34;http://www.brendankorourke.com/&#34; target=&#34;_blank&#34;&gt;http://www.brendankorourke.com/&lt;/a&gt; 博主&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以看出这么几件事：首先，这些领域内专家会互相关注对方的个人网站并通过这种方式互动；其次，看他们的讨论很有启发，比看书本上的干货更有意思，导致我现在每天花在统计博客上的时间比教科书多得多😂；再次，很多讨论虽然对问题是没营养的，但有助于我们了解一些学术界的风格或流派。在前沿领域由于知识不全，多数情况是无法达成共识的，但通过了解其流派风格会帮助你更全面的看问题。&lt;/p&gt;

&lt;p&gt;案例二：ggplot v.s. base plot&lt;/p&gt;

&lt;p&gt;R作为我们统计人最习惯的编程语言，同时拥有强大的绘图功能。其绘图包主要分为系统自带的基础模式和Hadley Wickham发明的ggplot。在ggplot刚刚兴盛起来时，著名统计博客&lt;a href=&#34;https://simplystatistics.org/&#34; target=&#34;_blank&#34;&gt;simply statistics&lt;/a&gt;的博主之一Jeef Leek发了一篇博客，名叫&lt;a href=&#34;https://simplystatistics.org/2016/02/11/why-i-dont-use-ggplot2/&#34; target=&#34;_blank&#34;&gt;我为什么不喜欢ggplot&lt;/a&gt;，当时引起轩然大波。一天之后，我最敬仰的数据科学家David Robison立马在他博客里发文，名叫&lt;a href=&#34;http://varianceexplained.org/r/why-I-use-ggplot2/&#34; target=&#34;_blank&#34;&gt;我为什么喜欢ggplot&lt;/a&gt;。但为了把故事讲的通透点，这位老兄追根溯源并展示了自己强大的R编程能力，对base R的质疑从作图速度到作图方便程度娓娓道来，行文令人拍案叫绝，在结论部分发起最后挑战，希望有更多人来质疑他的论据，可谓是上演了进攻-防守-再进攻的三部曲。&lt;/p&gt;

&lt;p&gt;最近Hadley Wickham提倡他所主张的tidyverse，也收到了一些老牌R用户的质疑。但Hadley作为一派之长，还是包着兼容并蓄的态度发表了以下的和解申明：&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;You can not use &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; without base R. It&amp;#39;s not a dichotomy. Pick the tools that make you most effective.&lt;/p&gt;&amp;mdash; Hadley Wickham (@hadleywickham) &lt;a href=&#34;https://twitter.com/hadleywickham/status/903103150332280832?ref_src=twsrc%5Etfw&#34;&gt;August 31, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;的确，科学研究就是会掺杂各种主观情感，但作为旁观者，我们可以从中去学习他们讨论问题的方法，例如 David Robison 和 Jeef Leek 的对自己观点的论证过程，虽然不如发表文章里那么逻辑完备，但思考步骤都是比较清晰的，而这个过程你在期刊论文中往往看不到，好比你看到的总是对方站在山顶但怎么爬上去的一般都不会写，但有时候这些看似琐碎的步骤却足够让你永远达不到那个高度。&lt;/p&gt;

&lt;p&gt;顺带一提， David Robinson 博士的博客上还友情链接了 Jeef Leek 教授的博客。我想说的是在国外是真真切切存在着通过博客的学术交流的，参与学者的水平也是相当强悍，而且不同于国内科研向公众号或博客满足于对论文的解读，这些博客上更多出现的是一种批判式讨论，而且夹杂了相当重的个人情绪，如果你打算阅读也是需要辨伪存真的，这本身对于提高科研思维也有帮助。&lt;/p&gt;

&lt;p&gt;其实类似的故事还有很多，你可以从这篇文章里出发用关键词去探索。我在前面的文本分析的文章中曾提到越是高端的论文，发表勘误的比率就越是很高，这说明前沿领域的研究不确定性是很高的，思想碰撞也很激烈。如果把推特等社交媒体上的各类花式吐槽也算进去，你会发现科研领域有很多烧脑的故事，各路参与者也从来都不缺名校光环跟牛文加持，阴谋诡计、解释掩饰、爱恨情仇等可能被小心翼翼地埋藏在数据与图表之中，虽然看懂需要比较高的门槛，但也正是这种门槛屏蔽了围观群众，上演一幕幕精彩绝伦但需要自行判断的书剑恩仇。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析那些事儿</title>
      <link>/zh/post/2018-03-04-pca/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2018-03-04-pca/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;目之所及，主成分分析应该是科研领域里最通用的一种数据分析手段。在相当长的一段时间里，我认为这种方法主要是用来进行探索分析的可视化手段与数据降维，但最近在帮根哥写一个监督式PCA的R包就把主成分分析又拎出来看了一下，这才意识到这个方法其实四通八达，可以把很多数据分析的概念连接起来。&lt;/p&gt;

&lt;h2 id=&#34;从线到点&#34;&gt;从线到点&lt;/h2&gt;

&lt;p&gt;首先还是回到一个最简单的场景，我有一堆数，想找出一个来做代表。从距离测量角度，就是找一个点让它离所有点最近，这就成了个优化问题，此时不同测量方法结论是不一样的。例如你考虑距离的绝对值最小，那你就会得到中位数；如果是差异的平方，求导后就是均值。回想下对一堆数找一个数，其实就是一种降维，从1维降低到0维。这里我们只考虑最小化差异的平方，那么求均值就是主成分分析把从1维降低到0维的应用场景。&lt;/p&gt;

&lt;h2 id=&#34;从面到线&#34;&gt;从面到线&lt;/h2&gt;

&lt;p&gt;现在复杂一点，我们设想一个二维平面，如果我们对其降维，那么降一维就是线，降两维就是点。而且我们可以确定降两维的那个点肯定就在降一维的线上，不然你这个降维就丧失了代表性。至于如何保障代表性，一般来说要交给数学家。那么这条线会通过所有点的均值，此时你应该想起来二维线性回归也通过这个点，那条线可以通过最小二乘得到，会不会就是我们要找的那条线？这个答案是否定的，最小二乘里最小化的是因变量到回归线的值，但是这里主成分分析最小化的是所有点到一条线的垂直距离，模型上细微的差别导致结果也会有区别，事实上求解过程也不对等。这一点在2位统计大牛的&lt;a href=&#34;https://www.youtube.com/watch?v=dbuSGWCgdzw&#34; target=&#34;_blank&#34;&gt;教学视频&lt;/a&gt;里也被强调。&lt;/p&gt;

&lt;h2 id=&#34;主成分分析的求解思想&#34;&gt;主成分分析的求解思想&lt;/h2&gt;

&lt;p&gt;虽然最小二乘回归线是高斯－马尔可夫定理下线性回归的最佳无偏估计，但主成分分析里二维降一维里那条线的求解思想并非回归到均值，常见有两种解释。第一种是寻找低维度空间，使投影点上到高维度点距离最近；另一种则是从统计角度寻找让点之间方差最大的方向映射，然后寻找跟这个方向正交的方向中方差最大的方向继续映射。从求解上，这两种解释都可转化成最优化问题，都是先标准化，然后求协方差矩阵，通过求导求特征向量跟特征值，那个方差最大的方向或距离最短子空间维度就是协方差矩阵里特征值最大的特征向量，剩下的方向或维度跟前面那个正交，再次找方差最大或距离最小即可。当然协方差矩阵不是一定要求的，如果你选用奇异值分解的套路就完全不用求。在这个求解策略下，解析解就是正交的，如果不是，那就不是主成分分析了。&lt;/p&gt;

&lt;p&gt;除此之外，理论上你也可以用隐藏变量模型迭代求解，不过有解析解不要用数值分析去逼近，而且有些矩阵运算可以进行分布式计算，这个在数据量大时是要特别考虑的。主成分分析求解上可以用矩阵是很大的优势，虽然理论上其概率解释并不完美。不同求解思想的多元分析方法其实背后都是有思想跟应用场景的，虽然理论上很多都是通用方法，但如果不适合你的数据就不要用。当前由于技术进步，之前很多很耗性能的方法目前都可以计算得到，如果搞科研我们要找那个最完美的，但在工业界应用可能更看重性价比。&lt;/p&gt;

&lt;h2 id=&#34;标准化&#34;&gt;标准化&lt;/h2&gt;

&lt;p&gt;如果我们进一步考察三维空间，那么我们的降维就首先是一个平面，然后是平面上的线，然后是线上的点。此时如果你对所有数据点乘2，那么很自然点、线、面的坐标位置都要变化，这样你就可以理解一个事实，那就是主成分分析对尺度是敏感的，所以一般来说都要对不同尺度／维度的测量进行标准化，否则你的映射会被大尺度的维度带跑偏。到现在为止，我们可以大概对主成分分析有个直观感受：将高维度空间的点映射到低纬度空间的点且要保证这些点之间的差异关系最大程度地保留，至于怎么保留，不同求解思想实际求解结果一致，都可以用矩阵运算，内含了进行转换或映射时要沿着正交的维度走（使用了正定阵），所以求解完矩阵就可以得到符合要求的低维度空间，而且低维空间是正交的。&lt;/p&gt;

&lt;h2 id=&#34;投影点的方差&#34;&gt;投影点的方差&lt;/h2&gt;

&lt;p&gt;主成分分析经常用来可视化，这里我们回到二维平面降维的场景仔细看看我们究竟可视化了什么。首先我们有一个二维点A，这个点投影到一维线上得到点B，这个点跟所有点的均值C连线就是到0维的投影。目前我们已知AC这个线，同时A到一维线的距离又要最小也就是垂直，这样A、B及C构成一个直角三角形。此时根据勾股定理BC这个距离最大，也就是一维到0维时所有投影点的距离之和最长，在这个方向中各点间方差最大程度保留，也就是找到了方差最大的方向，同时也找到了距离最近的点（&lt;a href=&#34;https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues&#34; target=&#34;_blank&#34;&gt;这个帖子&lt;/a&gt;讲的很清楚）。事实上，因为前面提到的直角三角形，每降低一次维度，点之间的距离比高维度都不可避免的减少，如果此时点聚到一起不一定相似度很高，但如果主成分占总方差比重比较大，那么这些点就很有可能十分相似。&lt;/p&gt;

&lt;h2 id=&#34;概率化的主成分分析&#34;&gt;概率化的主成分分析&lt;/h2&gt;

&lt;p&gt;主成分分析在求解上基本都走了矩阵运算的捷径，结果也是等价的。但这个过程不算是一个概率模型，因为可能产生不确定度的白噪音根本没出现在求解模型中。此时，我们应该意识到，这个子空间可能是某个概率模型的解，但如同我们只求了均值没求方差一样，似乎我们没有考虑模型的不确定度。这样我们需要从统计角度把主成分分析统一到基于统计学的数据分析中，这样也许会对将来构建相关假设检验模型有用，当然这也意味着我们可能不太方便再用矩阵运算来求解了。&lt;/p&gt;

&lt;p&gt;首先，我们对数据点进行假设，例如来自一个正态分布，那么主成分分析的问题就转化为求一个子空间，使得映射后的距离最小。让我们把这个映射关系描述成下面这样：&lt;/p&gt;

&lt;p&gt;$$
t = Wx + \mu + \epsilon
$$&lt;/p&gt;

&lt;p&gt;这里t是我们观察到的数据点，W是映射关系，维度不变可以理解成坐标轴旋转，x是映射后的点，$\mu$代表x的均值，$\epsilon$代表高斯随机变量。这样我们看到的点符合均值$\mu$，方差$WW^t + \psi$的正态分布，这里$\psi$代表了随机误差，如果我们不考虑这一项，那么主成分分析是完全可以用特征值跟特征向量求解的，此时我们默认那个误差项0方差。但是，实际场景中我们都很清楚每一个高维样本点都至少存在测量误差，这个误差的方差不是0，那么此时我们应该在模型中包含误差项，但一个很尴尬的问题是我们对这个误差一无所知。此时我们假定所有点的误差项来自于某一个方差统一的正态分布，然后有了这个限制条件就可以求解了。加入了这一部分后，主成分分析就可以进行假设检验了，例如某个点是否属于异常值。&lt;/p&gt;

&lt;h2 id=&#34;em算法&#34;&gt;EM算法&lt;/h2&gt;

&lt;p&gt;说到求解EM算法是绕不过去的，这个算法普适性比较强，存在隐藏变量的模型求解都可以用。主成分分析可以看作一种存在隐藏变量的模型，我们在低维空间看到的点背后是高维空间点但看不到，反之也成立。这样我们先随意假设一个新空间出来，这样我们就可以进行E阶段计算，也就是把看到的点投影到这个新空间上，然后计算距离。之后我们就可以进行M阶段，也就是最小化距离，这样就做出了一个比起始新空间距离更小的空间。然后再进行E阶段，M阶段，直到距离无法缩小。说白了就是模型不存在时先人工创造一个，然后不断按你的目标迭代让模型跟数据契合。在EM算法里，我们就可以很轻松把前面的方差项也扔进去一同优化，最后也能求解。这样概率化的主成分分析就有解了。不过这个算法具体实现存在很高的技巧性，我们吃现成就可以了。同时你会发现，其实EM算法思想可以用在很多不同模型参数求解上，马尔可夫过程、贝叶斯网络、条件随机场等有隐含变量的都可以用。&lt;/p&gt;

&lt;h2 id=&#34;因子分析&#34;&gt;因子分析&lt;/h2&gt;

&lt;p&gt;其实在更多资料中引入概率化的主成分分析主要是为了引入因子分析，因子分析跟概率化主成分分析最大区别在于其不限制误差来自方差相同的正态分布。这当然增加了计算难度，但其实因子分析对于解释这种隐藏结构其实比主成分分析更靠谱。但是，因子分析求解上不如主成分分析容易理解，需要通过一些方法来决定因子数或干脆使用者自己决定。此外，因子分析是可以进行预测的，目标就是潜在因子。从概率角度讲主成分分析自然也可进行预测，不过你得想清楚应用场景。同时，因子分析得到的成分也是正交的，这点跟主成分分析一致。正交的优点在于映射之间不相关，但不一定独立，如果数据分布需要独立因素就需要独立成分分析。&lt;/p&gt;

&lt;h2 id=&#34;独立成分分析&#34;&gt;独立成分分析&lt;/h2&gt;

&lt;p&gt;独立成分分析在独立成分符合正态分布时其实就是主成分分析，但当你独立成分并不来自正态分布时，独立成分分析就更有优势将其反推出来。因为独立跟相关是不同的，独立在统计学里比不相关约束条件更强，不相关不一定独立但独立一定不相关，独立因素间的互信息为0或者说高阶统计量上都不相关。最经典的应用就是鸡尾酒会问题(想起了陈老师当年的presesnatation😝)，在一个嘈杂的场景里很多人都在说话，你在固定位置放了几个麦克风，这样麦克风收集到的就是多种声音的混合，现在你需要把混音中每个人的声音提取出来。此时你要用主成分分析，你会得到所有人声音的共性，但独立成分分析就可以分辨出每个个体，或者说潜在变量，所以你也猜到了，EM算法也可以求解独立成分分析。需要注意的是独立成分分析不管降维，基本你设定分多少个就有多少个。但不论主成分分析、因子分析还是独立成分分析，本质上都是线性模型的结构，也就是所谓的主成分、因子、独立成分都是原始数据的线性组合。&lt;/p&gt;

&lt;h2 id=&#34;聚类-共性-压缩-降噪&#34;&gt;聚类／共性／压缩／降噪&lt;/h2&gt;

&lt;p&gt;有些论文用主成分分析搞聚类画圈圈来说明样品间存在内在共性。这个在环境分析中比较常见，因为环境分析通常同时测定上百种化合物，前面提到低维映射里最大程度保留了样品点的差异，此时映射到一起就有可能说明那些样品污染特征接近，便于探索来源或环境过程。实际上此时不一定需要局限在主成分分析，可以直接用聚类分析等统计模型。&lt;/p&gt;

&lt;p&gt;很多人搞不清楚特征值、特征向量还有载荷等的细节，所以主成分分析就被用成了降维画图工具，但其实这个探索分析是针对背后隐藏变量的，具体到主成分分析就是共性。还是举个例子来说吧，我有100个样品，每个样品测了1000个指标，现在我就有了个100$\times$1000的矩阵，通过主成分分析我得到了100$\times$250的矩阵，这个矩阵包含了原数据95%的方差。好了，现在我问你，这250个新指标是什么？对，特征向量，特征向量就是新投影方向，投影可以看作隐含共性。特征值又是什么，共性的权重，越大代表越重要，毕竟可以代表更多的方差。那么载荷又是什么，大概可以理解成原来1000个指标对250个新指标的贡献。那么进行分析时我们在样本和指标之间多了一个共性层，一方面减少了数据维度，另一方面算是提取了指标间不相关的共性（但不一定独立，切记）。对于多出来的共性层，我们同时知道样品在这些共性上的分布，也知道每个指标对共性的分布，常见的biplot就可以同时绘制样品点跟指标在两个最重要共性上的分布，一目了然。此时专业知识就要上场了，我们可能会通过指标相互作用发现共性背后对应隐含因素的物理含义，也可以发现某种分离样品的共性背后暗示的样品潜在来源。总之，多了一个共性层后，我们可以研究的机理就更明显了，例如自然语言处理里可以用其寻找文本主题，基因组学里可以用来寻找基因模块等。但需要提醒的是，这个“共性”并不代表客观规律，只是一种线性变换后的结果，如果跟实际想研究的因素不对应还不如直接上回归分析。&lt;/p&gt;

&lt;p&gt;主成分分析或者说实现主成分分析的奇异值分解的另一个应用就是可以用来压缩数据，上面的例子中100*1000的数据空间如果赶上稀疏矩阵十分浪费，此时就可以用奇异值分解压缩存储空间。从信号处理的角度去看，主成分分析跟傅立叶变换等变换过程本质上都是用一组新信号替代原来信号，由于一般认为信号方差高于噪音方差，通过变换时保留主成分或特定频谱，我们同时可以做到降噪。图形处理也可以用，而所有数据其实都可以用图形展示，那么作为图形降噪的主成分分析背后跟数据降噪是可以联系到一起的，特别环境痕量分析中的降噪。结合前面的结构重构、方差保留等性质，其实哪怕你就只会主成分分析，想明白了能做的事也很多。&lt;/p&gt;

&lt;p&gt;你只是需要一点想象力。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>知识的压缩与解压</title>
      <link>/zh/post/2018-03-03-uncertainty/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2018-03-03-uncertainty/</guid>
      <description>&lt;p&gt;昨晚张少典学长来哥大介绍了他的公司，以及人工智能在中国医疗届的高速发展。整个讲座干货满满，但让我印象最深的还是他关于数据预处理重要性的论述。的确，中国医疗数据数量远超于其他任何国家，但其质量就不敢让人恭维。于是乎大量时间都得花在整理、标准化数据上，而传统的机器学习方法就在整个过程中就不那么重要啦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/b33hrrHb0DmOlmR1iI66p_ArSosoq5zNOwVZndnd9e1BRYCeH7skWKzJPSwEbKjratqlhHQBgr6qYextFNHvxU8rwgyBsnk7eZV3sDs8YvX3N7VigKcKp7UVEvDFwuPg1bbOyq4GwzS46ikkLw1PYlN-Jv9O7Yr6BBkF3pzTxl9JiISI5EuhckE0oDIbinLD6nVNo6AEN8iCU1ANn1kKvnx0GwBDk93kio-r_x4XmfPPBbJs1cdU2KsMAiwLQFE-mLB5IhWnVdQSXK9aaI3kh3Xhma2W92zl5EtwZNOS2OPeqKrBymh9Ij96QP2958f4AiTOtYI6iOfyk7boXpUKWvqMY61r_x7mUxkXpHp0WGkxz6dTk0zgxNLM-WZveGrRa4ORPWqPa_f42SLzcHMSJ55xXhI7ppqcwysmLclEl-V9ZwF80mgQQyLBJRsSgglZ35lRHMPo4NE8WX4e-l5AGclKTCW22gMwEXepqBuDh-APt4l-NT0xgZggZ9HlS01TpdekohLVi0WpARG_jPSWcqyZaaFi3Ge_egQGnA3yQ32S4XfKrQLwDaiZ4Bi8SDH6Z0Zz6TapTMo1WyJPjv4bPLF3xQwabAZTUqJzH8OXdxKc-ATcJQK-mVH_C0Q98DyMDaJyKvnu61GkWCLbOnX0mqefA6JASg3o=w1142-h856-no&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;因为自己在数据预处理方面也是一个初学者，目前没啥经验之谈，所以我想分享一个由此引申出来的问题，那就是信息（知识）预处理方面的感悟。&lt;/p&gt;

&lt;p&gt;说实话现在我们有了Google等搜索引擎之后，知识的本体记忆变得不那么重要而快速搜集资料并内化学习的能力越来越成为现代社会的必备生存技能。这种技能早期被称为个人知识管理，对于每个人而言都是不一样的，有的崇尚工具，有的依托记忆，其实本质上都是一套方法论体系。&lt;/p&gt;

&lt;p&gt;邮件日记、GTD、番茄时钟、康奈尔笔记法、晨间日记、紧急-重要四象限法则、甘特图、香肠战术……各种体系都有着自己的拥趸与应用场景，一般人从无到有时任何体系都会很大的改变工作效率，但一段时间后可能每种方法都不那么灵光了，方法本身可能没什么问题，更多的不协调可能是使用者本身过于依赖方法来实现目标而没有真正思考目标本身究竟是否适合方法背后的假设。&lt;/p&gt;

&lt;p&gt;就算我列出每种方法的适用场景恐怕也不能解决方法失灵问题，因为任何机械查表式的套用都有着大力丸跟万金油的嫌疑，属于思想上的懒惰，多半还会遇到使用者对场景的混淆。这方面我的建议是先随意找一种方法论完全吃透，然后在实践中发现其局限性，然后能动改进，形成自己的套路。&lt;/p&gt;

&lt;p&gt;人脑就是一个信息输入-输出系统，且能够即时反馈出的信息量有生理上限。知识管理的核心就是在有限的信息容量里装入尽可能多的信息，其实就是输入信息的压缩与解压方法。&lt;/p&gt;

&lt;p&gt;由此可以设想如果信息间是零散独立的，那么并不存在压缩信息的可能。不过如果很多信息可以抽象成一条信息，那么大脑的负担就可以迅速降低，但每次抽象都会损失信息量。同样，在使用或输出信息时，需要解压为具象的语言，此时知识需要增加信息量。&lt;/p&gt;

&lt;p&gt;当前流行的知识预处理更多关注了信息了整理与抽象，但对知识的使用缺少关注。我在很多 MOOC 笔记网站看到了很多归纳十分精彩笔记，但是感觉过于专著笔记的逻辑结构了，这样在严谨的逻辑框架下很难使用这些知识。有时候理科思维是需要工科思维来落地的，最简单的方法就是进行知识输出的场景训练。&lt;/p&gt;

&lt;p&gt;写作、演讲与教学（TA）是目前我认为最好的让知识解压的训练方法。当你看到一个好的创意或工具，简单的收藏与归类并不能让知识变成你的，得用作者的思路重新走一遍，想象自己提出了这个方法并要教给别人，然后在教的过程体会方法局限性。这个过程知识从抽象变具象，增加的信息量就是你使用这个知识的自由度。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You do not really understand something unless you can explain it to your grandmother.                                            - Albert Einstein&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实写作好比一个翻译过程，把别人的话变成自己的话，同时包括了信息输入与输出过程。以我的经验，只有走这么一出，这个知识点才会真正被掌握，随时可以自由使用出来。从这个角度看，应试教育最大的问题可能是学生可通过技巧在不理解知识的情况下取得高分，等到了无法直接套公式的应用场景就只能抓瞎了。&lt;/p&gt;

&lt;p&gt;其实这个问题最严重的情况是出在科技论文的写作上，以我有限的文献阅读经验，经常发现作者在使用一些方法时只是单纯山寨了其他论文的方法模版，也就是通过排列组合把自己的研究嵌入到一个固定的模式中去，然后就认为一劳永逸了。这样的论文在逻辑严谨性上会继承已发表论文的所有问题，但很难产生新的知识，因为没有新思考来增加信息量。&lt;/p&gt;

&lt;p&gt;新的问题总是需要新的知识与思索来论证，论文不是让你阐述事实而是论述新知识，所有的手段都是在说服读者认同你的观点，很难想象在根本不理解方法的情况下单纯追求方法本身有多大意义，特别是数据分析方法。我们不仅需要可重复性的研究，更需要有思考的解压式研究，这部分才是科研里最有意思的部分。&lt;/p&gt;

&lt;p&gt;回到知识预处理 ，单纯追求效率意义同样不大，一年读几百本书或几分钟读一本书都是在刷数据，舍本逐末。知识管理最好的验证不是数据，而是不断问自己我的知识与思考自由度是否更大了？从知识的应用角度进行验证是最简单的，或者说带着问题去求知是最有效的，让知识压缩进头脑，然后解压成行动，每一步都有思考反馈才是对知识本身的尊重。&lt;/p&gt;

&lt;p&gt;现代社会的压力让所有人都焦虑，最开始是害怕自己落后，后来是害怕孩子落后。解决焦虑的方法是购买确定性，例如学个高薪专业（CS）、买个学区房、考个证、嫁个有钱人… 这也是拒绝思考的表现，所有现阶段的确定性都存在未来的不确定性，不是说你花了钱就安全了，最多是感到安全而已。对知识的追求也是如此，很多人只是为了缓解焦虑而求知，没用的，只有理解并拥抱不确定性才能真正解决焦虑问题，体会知识本身乐趣。&lt;/p&gt;

&lt;p&gt;如果你的求知目标是确定的，例如拿到生统PhD迈入学术界，那这个问题就是 AI 可解可优化的，那么只要计算能力在提高，总有一天 AI 可以帮助你，然后替代你。但 AI 是无法提出并优化求解一个不存在或矛盾的问题的，不信你可以问 AI 电车难题，是要一个胖子死还是四个瘦子死，AI 解不了这类逻辑悖论或伦理问题，写不出 loss function或只能写一种原则下的 loss function或加权几种原则下 loss function，原则还是要人提出来。这类充满矛盾与不确定性的原则才是应该花费时间去思考的，焦虑未来的不确定性实在多余，未来总是有不确定性的，正是有不确定性人才不是机器。&lt;/p&gt;

&lt;p&gt;说到不确定性，我又想起了Jenny Bryan在&lt;a href=&#34;https://www.statschat.org.nz/2017/12/15/jenny-bryan-you-need-a-huge-tolerance-for-ambiguity/&#34; target=&#34;_blank&#34;&gt;访谈&lt;/a&gt;中的一段话，好久不读，是时候拿出来再温习一遍了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What skills and attributes make a good data scientist?&lt;/strong&gt; I think being naturally curious, doing something for the sake of answering the question versus a “will-this-be-in-the-test?” mentality – just trying to do the minimum.&lt;/p&gt;

&lt;p&gt;You need a huge tolerance for ambiguity. This is a quality I notice that we’re spending a lot of time on in our Master of Data Science programme at UBC. Half the students have worked before and about half are straight out of undergrad, and the questions they ask us are so different. The people straight out of undergrad school expect everything to be precisely formulated, and the people who’ve worked get it, that you’re never going to understand every last thing; you’re never going to be given totally explicit instructions. Figuring out what you should be doing is part of your job. So the sooner you develop this tolerance for ambiguity [the better] – that makes you very successful, instead of waiting around to be given an incredibly precise set of instructions. Part of your job is to make that set of instructions.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>统计学习中的 Bias-Variance Trade-Off</title>
      <link>/zh/post/2018-02-07-bias-variance/</link>
      <pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2018-02-07-bias-variance/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;最近在复习一门统计学习的公开课，由&lt;a href=&#34;http://statweb.stanford.edu/~tibs/ElemStatLearn/&#34; target=&#34;_blank&#34;&gt;ESL&lt;/a&gt;的两个作者Trevor Hastie与Robert Tibshirani讲授，不过使用的教材是ESL的简化版—&lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34; target=&#34;_blank&#34;&gt;《An Introduction to Statistical Learning》&lt;/a&gt;。虽说是简化版但写作风格明显是面对不同群体的，ISL偏应用，ESL更侧重数学原理解析，更抽象些。但这两本书的核心都是围着统计学习转的，而统计学习的一个核心论题就在Bias-Variance的取舍上。&lt;/p&gt;

&lt;p&gt;初涉统计学习最直观的图形就是过拟合与欠拟合训练集的模型对测试集残差的倒U型曲线，解释上也会偏重说明过拟合问题，下面就从原理层解释下这个问题：&lt;/p&gt;

&lt;p&gt;首先，我们知道统计学习实际上关注的是模型与现实问题，将现实的数据转化提炼出抽象的模型用来预测或做关系推断。那么先把无监督学习跟关系推断放一边，我们面对的是$Y = f(X) + \epsilon$ 的回归预测问题，这里$f(x)$是理想模型。那么从公式上看我们可以想像，即使是理想模型，也存在$\epsilon$，这是由Y背后的分布决定的，即预测值存在于一个范围而不是定值，通过其出现的概率密度函数，我们可以得到取值的变动范围。&lt;/p&gt;

&lt;p&gt;其次，要清楚实际的统计学习过程是借助$\hat f(x)$来进行的，而$\hat y = \hat f(x)$,也就是每一种建模方法实际对应了一种对理想$f(x)$的估计。那么理想$f(x)$是没有bias的，但实际的建模过程是存在bias的，这个bias是可以有方向的，用bias表示。而在建模过程中模型对理想模型$f(x)$也存在变动范围，受训练集影响大的模型变动大，Variance就高，这就是过拟合的来源。&lt;/p&gt;

&lt;p&gt;再次，不要忘记我们建模实际关心的不是$\hat y$而是$y$，那么在评价建模过程时我们引入一个统计量$MSE$来衡量预测值与实际值的差异，有$MSE = E(Y - \hat Y)^2 = [f(x) - \hat f(x)]^2 + Var(\epsilon)$，这个公式的证明很重要，但基本就是一个代数过程。从结果上看，差异的来源由两部分构成，一部份源于模型的bias，另一部分源于$y$自身的bias，前者想想办法可以降低，后者由你响应变量的自身属性决定，基本不可约了。&lt;/p&gt;

&lt;p&gt;最后，关注下$[f(x) - \hat f(x)]^2$，这一部份实际由两部分构成，就是上面提到的Variance与bias。这两部分都低的建模方法更接近实际，但统计学习的一些方法是面临一个取舍问题的，那就是 Bias-Variance Trade-Off。&lt;/p&gt;

&lt;p&gt;这里用交叉验证中的Leave-One-Out CV (LOOCV)与K-Fold CV (KFCV)来说明下这个问题，LOOCV的核心在于留下一个作为验证集，用其余的数据建模，建n个模型求验证集的$MSE$来进行模型评价。KFCV则将数据分为k份，大部份拿来建模，留一部分拿来验证。从Bias角度，由于LOOCV实际上使用了近乎所有数据，而KFCV会有$\frac{n}{k}$部分不参与建模，所以相同训练集条件下LOOCV的Bias会很小。但从Variance角度，使用的数据基本相同，过于依赖同一套数据，在测试集上会表现模型Variance偏大，反观KFCV，Bias可能由于训练不够偏高，但构建的模型独立性较好，抵消掉了对同一数据集的依赖，得到的模型在Variance上会相对小。这样我们会看到一个有意思的现象，伴随模型复杂度的提高，LOOCV与KFCV的$MSE$实际是差不多的，但其中Bias与Variance的组成是不一样的，从预测效果上看，可能Variance低的模型更具吸引力，因此KFCV会比LOOCV更受欢迎。&lt;/p&gt;

&lt;p&gt;另一个解释的角度是从残差相关度上理解的，如果预测值与实际值的差距在训练集上高度相关，这样如果假设建模过程的Variance对某一特定模型是固定的，那么训练集上的高度相关就反过来导致测试集变动范围增大。此外，数据生成过程如果存在自变量的相关，其势必造成参数标准误估计的偏低，进而影响模型的稳健性，这一过程还是比较常见的：时间序列分析。&lt;/p&gt;

&lt;p&gt;总之，Bias-Variance Trade-Off 贯穿在近乎每一个建模过程之中，统计学习的一个全局观就体现在其目标针对的是全局最优的理想模型。从这个角度出发，技术上的改进本质上是发现并平衡掉各种实际情况中的不完美或不理想。在这个层面上用好数学工具是可以从根上将问题分析透彻的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>码农的诗意生活</title>
      <link>/zh/post/2017-12-30-purrr/</link>
      <pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2017-12-30-purrr/</guid>
      <description>&lt;p&gt; 今天R代码一行没写，倒是删掉了一百行。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>又要毕业了？！</title>
      <link>/zh/post/2017-12-28-graduation/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/zh/post/2017-12-28-graduation/</guid>
      <description>&lt;p&gt;昨天在地铁里刷微博，里面有胡适的一篇文章，《赠与今年毕业的大学生》（《独立评论》1932年7月3日），近八十年过去了，这篇文章仍然给人启发，即使是对我这种毕业不久又快要毕业的**——现在，都不知道用什么词汇描述自己了，噫？！&lt;/p&gt;

&lt;p&gt;胡适说，大伙毕业之后，可走的路无非有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;绝少数的人在国内，或者出国，继续做研究；&lt;/li&gt;
&lt;li&gt;少数人能找到相当的工作；&lt;/li&gt;
&lt;li&gt;此外还有做官、办党、革命三条路；&lt;/li&gt;
&lt;li&gt;还有就是在家享福或者失业闲居。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;虽然每年毕业生继续读研读博的人不在少数，但大多数人的归宿仍然是非学术性工作，故第一条，做研究的，现在还是少数。胡适说只有少数人能找到相当的工作，大概说的是与自己兴趣/能力相当的工作，或者体面的工作，这个不好统计，每个人都有自己的想法，不过外表光鲜内心苦楚的所谓体面工作在当下不在少数，至于与自己能力相当，呵呵，据说大部分人对自己的判断都有拔高的倾向。&lt;/p&gt;

&lt;p&gt;胡适接着说，除了继续求学，其他几条路，都有堕落的危险。这不是说读研读博就没有堕落，只是说在学校里堕落，大伙都能容忍。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;人生的道路上满是陷阱，堕落的方式很多。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;胡适总结出主要的两种，一是容易抛弃学生时代的求知识的欲望，二是容易抛弃学生时代的理想的人生的追求。&lt;/p&gt;

&lt;p&gt;怎么说？胡适还是有些理想主义气质，或者那个时代的大学还存有某些理想主义气质。我们的高校系统，一不能促进学生的求知欲望，二不能鼓励学生的理想主义追求。但高校里还是能够保有一些有求知欲有理想的年轻人，相互期许，并给旁边人以鼓舞或感动，到了社会，这一丁点即使是贫瘠的土壤也没了：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;到了实际社会里，往往所用非所学，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里，即使向来抱有求知识学问的决心的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的；书籍，试验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人所能容易办到的。&lt;/p&gt;

&lt;p&gt;少年人初次与冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事，渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就熔化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲曳兵而走，或者做了恶力的俘虏。你在那俘虏牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦！从此以后，你就甘心放弃理想人生的追求，甘心做现成社会的顺民了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;接着胡适推荐了三个法子防御这两方面的堕落：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;总得时时寻一两个值得研究的问题！&lt;/li&gt;
&lt;li&gt;总得多发展一点非职业的兴趣！&lt;/li&gt;
&lt;li&gt;你总得有一点信心。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;听着是卑之无甚高论。感兴趣的是第二条，也经常跟朋友说过。是篇好文章，做回文抄公：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;离开学校之后,大家总得寻个吃饭的职业。可是你寻得的职业未必就是你所学的,或者未必是你所心喜的,或者是你所学而实在和你的性情不相近的。在这种状况之下,工作就往往成了苦工,就不感觉兴趣了。为糊口而作那种非&amp;rdquo;性之所近而力之所能勉&amp;rdquo;的工作,就很难保持求知的兴趣和生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。一个人应该有他的职业,又应该有他的非职业的玩艺儿,可以叫做业余活动。凡一个人用他的闲暇来做的事业,都是他的业余活动。往往他的业余活动比他的职业还更重要,因为一个人的前程往往全靠他怎样用他的闲暇时间。他用他的闲暇来打麻将,他就成了赌徒;你用你的闲暇来做社会服务,你也许成个社会改革者;或者你用你的闲暇去研究历史,你也许成个史学家。你的闲暇往往定你的终身。&lt;/p&gt;

&lt;p&gt;英国十九世纪的两个哲人,密尔终身做东印度公司的秘书,然而他的业余工作使他在哲学上、经济学上、政治思想史上都占一个很高的位置;斯宾塞是一个测量工程师,然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人,几乎没有一个不是善用他的闲暇时间的。特别在这个组织不健全的中国社会,职业不容易适合我们性惰,我们要想生活不苦痛或不堕落,只有多方发展业余的兴趣,使我们的精神有所寄托,使我们的剩余精力有所施展。有了这种心爱的玩艺儿,你就做六个钟头的抹桌子工夫也不会感觉烦闷了,因为你知道,抹了六点钟的桌子之后,你可以回家去做你的化学研究,或画完你的大幅山水,或写你的小说戏曲,或继续你的历史考据,或做你的社会改革事业。你有了这种称心如意的活动,生活就不枯寂了,精神也就不会烦闷了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;你可以在&lt;a href=&#34;http://www.douban.com/group/topic/4748460/&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;看到全文，与各位大神共勉！&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
